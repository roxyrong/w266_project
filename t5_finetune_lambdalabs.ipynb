{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/roxyrong/w266_project/blob/main/t5_finetune_text_to_sql.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "rZJNzUdXJLqb"
   },
   "outputs": [],
   "source": [
    "%%capture \n",
    "\n",
    "!pip install transformers\n",
    "!pip install sentencepiece\n",
    "!pip install accelerate -U\n",
    "!pip install datasets\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "NrxT7ulwR19d"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "project_path = ''\n",
    "sys.path.append(project_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "SPQuxy9_H0HP"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Dict, List\n",
    "import subprocess\n",
    "import collections\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "1cEvN9aDomCK"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for evaluation\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "bg4qdsH5JKX5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "zva6eKoPH7j1"
   },
   "outputs": [],
   "source": [
    "# datasets\n",
    "with open('spider/train_spider.json', 'r') as f:\n",
    "    train_spider = pd.read_json(f)\n",
    "with open('spider/train_others.json', 'r') as f:\n",
    "    others_spider = pd.read_json(f)\n",
    "with open('spider/dev.json', 'r') as f:\n",
    "    dev_spider = pd.read_json(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "3iYPW9ODH80T"
   },
   "outputs": [],
   "source": [
    "# load schema for all tables\n",
    "with open('spider/tables.json', 'r') as f:\n",
    "    schema_df = pd.read_json(f)\n",
    "\n",
    "# db_dir = f\"spider/database\"\n",
    "# schema_dict = {}\n",
    "# column_names_dict = {}\n",
    "# for idx, row in schema_df.iterrows():\n",
    "#     db = row['db_id']\n",
    "#     db_path = os.path.join(db_dir, db, db + \".sqlite\")\n",
    "#     schema = str(get_schema(db_path))\n",
    "#     schema_dict[db] = schema\n",
    "#     column_name = json.loads(schema.replace(\"'\", \"\\\"\"))\n",
    "#     column_names_dict[db]= {\"table_id\": list(column_name.keys()), \"column_name\": schema}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "GE6FOe8wQ4QV"
   },
   "outputs": [],
   "source": [
    "def _get_schema_string(table_json):\n",
    "    \"\"\"Returns the schema serialized as a string.\"\"\"\n",
    "    table_id_to_column_names = collections.defaultdict(list)\n",
    "    for table_id, name in table_json[\"column_names_original\"]:\n",
    "        table_id_to_column_names[table_id].append(name.lower())\n",
    "        tables = table_json[\"table_names_original\"]\n",
    "\n",
    "    table_strings = []\n",
    "    for table_id, table_name in enumerate(tables):\n",
    "        column_names = table_id_to_column_names[table_id]\n",
    "        table_string = \" | %s : %s\" % (table_name.lower(), \" , \".join(column_names))\n",
    "        table_strings.append(table_string)\n",
    "\n",
    "    return \"\".join(table_strings)\n",
    "\n",
    "schema_dict = {}\n",
    "for idx, row in schema_df.iterrows():\n",
    "    db_id = row['db_id']\n",
    "    schema = _get_schema_string(row)\n",
    "    schema_dict[db_id] = schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "gZPZvrTWH-LC"
   },
   "outputs": [],
   "source": [
    "# shuffle the dataset\n",
    "\n",
    "train_spider = train_spider.iloc[np.random.permutation(train_spider.index)].reset_index(drop=True)\n",
    "others_spider = train_spider.iloc[np.random.permutation(others_spider.index)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Ja3AjYzL0Ao"
   },
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "an2hxxSuH_6g"
   },
   "outputs": [],
   "source": [
    "# construct prompt\n",
    "\n",
    "prefix = 'translate English to SQL:'\n",
    "\n",
    "train_spider['schema'] = train_spider['db_id'].map(schema_dict)\n",
    "train_spider['prompt'] = prefix + train_spider['question'] + '\\nDatabse schema is ' + train_spider['schema']\n",
    "others_spider['schema'] = others_spider['db_id'].map(schema_dict)\n",
    "others_spider['prompt'] = prefix + others_spider['question'] + '\\nDatabse schema is ' + others_spider['schema']\n",
    "dev_spider['schema'] = dev_spider['db_id'].map(schema_dict)\n",
    "dev_spider['prompt'] = prefix + dev_spider['question'] + '\\nDatabse schema is ' + dev_spider['schema']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "hizv4iaEIg1T"
   },
   "outputs": [],
   "source": [
    "def preprocess_data(text_pair, tokenizer, max_length=128):\n",
    "    orig_text, target_text = text_pair\n",
    "    orig_encoded = tokenizer.batch_encode_plus(\n",
    "        [orig_text],\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    orig_input_ids = orig_encoded['input_ids'][0]\n",
    "    orig_attention_mask = orig_encoded['attention_mask'][0]\n",
    "\n",
    "    target_encoded = tokenizer.batch_encode_plus(\n",
    "        [target_text],\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    label_ids = target_encoded['input_ids'][0]\n",
    "\n",
    "    return {'input_ids': orig_input_ids,\n",
    "            'attention_mask': orig_attention_mask,\n",
    "            'labels': label_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "_9IiV9LXIkkD"
   },
   "outputs": [],
   "source": [
    "class TranslationDataIterator:\n",
    "\n",
    "    def __init__(self,\n",
    "                 df,\n",
    "                 tokenizer,\n",
    "                 max_load_at_once,\n",
    "                 max_length=128,\n",
    "                 shuffle=True):\n",
    "\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.n_examples = len(df)\n",
    "        self.max_load_at_once = max_load_at_once\n",
    "        self.max_length = max_length\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        # Initialize row order, call on_epoch_end to shuffle row indices\n",
    "        self.row_order = np.arange(1, self.n_examples+1)\n",
    "        self.on_epoch_end()\n",
    "\n",
    "        # Load first chunk of max_load_at_once examples\n",
    "        self.df_curr_loaded = self._load_next_chunk(0)\n",
    "        self.curr_idx_in_load = 0\n",
    "\n",
    "    def _load_next_chunk(self, idx):\n",
    "        load_start = idx\n",
    "        load_end = idx + self.max_load_at_once\n",
    "\n",
    "        # Indices to skip are the ones in the shuffled row_order before and\n",
    "        # after the chunk we'll use for this chunk\n",
    "        self.df_curr_loaded = self.df.iloc[load_start:load_end].sample(frac=1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_examples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.df_curr_loaded is None or self.curr_idx_in_load >= len(self.df_curr_loaded):\n",
    "            self._load_next_chunk(idx)\n",
    "            self.curr_idx_in_load = 0\n",
    "\n",
    "        text_pair = self.df_curr_loaded[['prompt', 'query']].values.astype(str)[self.curr_idx_in_load]\n",
    "        self.curr_idx_in_load += 1\n",
    "\n",
    "        item_data = preprocess_data(\n",
    "            text_pair,\n",
    "            self.tokenizer,\n",
    "            self.max_length\n",
    "        )\n",
    "\n",
    "        return item_data\n",
    "\n",
    "    def __call__(self):\n",
    "        for i in range(self.__len__()):\n",
    "            yield self.__getitem__(i)\n",
    "\n",
    "            if i == self.__len__()-1:\n",
    "                self.on_epoch_end()\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            self.row_order = list(np.random.permutation(self.row_order))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TmEu_mBcL8zW"
   },
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ArkzDDRD4u2H"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_path: results/t5-base_fine_tuned_1\n",
      "model_path: results/t5-base_fine_tuned_1/t5-base_fine_tuned_1\n"
     ]
    }
   ],
   "source": [
    "model_name = \"t5-base\"\n",
    "technique = \"fine_tuned\"\n",
    "version = 1\n",
    "\n",
    "folder_name = f\"{model_name}_{technique}_{version}\"\n",
    "train_path = f\"results/{folder_name}\"\n",
    "model_path = train_path + f'/{folder_name}'\n",
    "last_check_point = train_path + f'/checkpoint-8500'\n",
    "\n",
    "print('train_path:', train_path)\n",
    "print('model_path:', model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "U2FgTQubLjp9"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "gZ4pWEZ8ImOU"
   },
   "outputs": [],
   "source": [
    "max_length = 512\n",
    "max_load_at_once = 100\n",
    "\n",
    "train_data_iterator = TranslationDataIterator(\n",
    "    df=train_spider,\n",
    "    tokenizer=tokenizer,\n",
    "    max_load_at_once=max_load_at_once,\n",
    "    max_length=max_length\n",
    ")\n",
    "\n",
    "valid_data_iterator = TranslationDataIterator(\n",
    "    df=others_spider,\n",
    "    tokenizer=tokenizer,\n",
    "    max_load_at_once=max_load_at_once,\n",
    "    max_length=max_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "d7r9oQcVInfU"
   },
   "outputs": [],
   "source": [
    "batch_size = 12\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    train_path,\n",
    "    evaluation_strategy='epoch',\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "fWGwZQFhIo0_"
   },
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_data_iterator,\n",
    "    eval_dataset=valid_data_iterator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gr-QshK9IqBj"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='59' max='2920' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  59/2920 00:44 < 36:53, 1.29 it/s, Epoch 0.10/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# start from scratch\n",
    "trainer.train()\n",
    "\n",
    "# start from a checkpoint\n",
    "# trainer.train(resume_from_checkpoint= last_check_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iz6CbDO2Vd6k"
   },
   "outputs": [],
   "source": [
    "trainer.save_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Cp4TIpG27hA"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QwK5m6fAZ_vM"
   },
   "outputs": [],
   "source": [
    "finetune_model = T5ForConditionalGeneration.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q4perd2zXmzG"
   },
   "outputs": [],
   "source": [
    "# evaluate\n",
    "max_length = 128\n",
    "\n",
    "inputs = tokenizer.batch_encode_plus(\n",
    "        list(dev_spider['prompt']),\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "output_tokens = finetune_model.generate(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    max_length=128\n",
    ")\n",
    "\n",
    "outputs = [tokenizer.decode(i, skip_special_tokens=True) for i in output_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3r5R4IuOgI7r"
   },
   "outputs": [],
   "source": [
    "with open(f'{folder_name}/predicted_result.txt', 'w') as f:\n",
    "    for idx, output in enumerate(outputs):\n",
    "        db_id = dev_spider.iloc[idx]['db_id']\n",
    "        f.write(output + '\\t' + db_id + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nv7ZpIoaajtw"
   },
   "outputs": [],
   "source": [
    "# evaluate results\n",
    "eval_path = f\"third_party/spider/evaluation.py\"\n",
    "gold = f\"third_party/spider/evaluation_examples/gold_example.txt\"\n",
    "pred = f\"{folder_name}/predicted_result.txt\"\n",
    "db_dir = f\"spider/database\"\n",
    "table = f\"spider/tables.json\"\n",
    "etype = \"all\"\n",
    "\n",
    "cmd_str = f\"python3 \\\"{eval_path}\\\" --gold \\\"{gold}\\\" --pred \\\"{pred}\\\" --db \\\"{db_dir}\\\" --table \\\"{table}\\\" --etype {etype} \"\n",
    "result = subprocess.run(cmd_str, shell=True, capture_output=True, text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hh4yyml1jMbY"
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "pprint.pprint(result.stdout[-4633:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
